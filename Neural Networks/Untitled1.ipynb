{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "0BZ0h9pQ8ZKe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "278540b9-d73b-4bb9-d8fc-cf2284a6b8e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import pickle\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.transforms import ToTensor, Normalize, Compose\n",
        "\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_path = '/content/drive/My Drive/100/'\n",
        "os.chdir(dataset_path)"
      ],
      "metadata": {
        "id": "4SOB-oQu_47M"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!tar -xvzf cifar-10-python.tar.gz"
      ],
      "metadata": {
        "id": "lStFhDAUEL_4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a7c0a2d-71f5-492a-c2ec-c765d1fb2c95"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cifar-10-batches-py/\n",
            "cifar-10-batches-py/data_batch_4\n",
            "cifar-10-batches-py/readme.html\n",
            "cifar-10-batches-py/test_batch\n",
            "cifar-10-batches-py/data_batch_3\n",
            "cifar-10-batches-py/batches.meta\n",
            "cifar-10-batches-py/data_batch_2\n",
            "cifar-10-batches-py/data_batch_5\n",
            "cifar-10-batches-py/data_batch_1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.listdir('.')"
      ],
      "metadata": {
        "id": "E__a8bsCETlk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3045ade8-147a-4bae-b501-4c72becac531"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['cifar-10-python.tar.gz', 'cifar-10-batches-py']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def unpickle(file):\n",
        "    with open(f'cifar-10-batches-py/{file}', 'rb') as fo:\n",
        "        dict = pickle.load(fo, encoding='bytes')\n",
        "    return dict\n"
      ],
      "metadata": {
        "id": "FlT3XYD5ALiv"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = []\n",
        "train_labels = []\n",
        "\n",
        "# Load and append all training batches\n",
        "for i in range(1, 6):\n",
        "    batch = unpickle(f'data_batch_{i}')\n",
        "    train_data.append(batch[b'data'])\n",
        "    train_labels += batch[b'labels']\n",
        "\n",
        "# Convert lists to numpy arrays for convenience\n",
        "train_data = np.vstack(train_data).reshape(-1, 3, 32, 32).transpose(0, 2, 3, 1)\n",
        "train_labels = np.array(train_labels)\n",
        "\n",
        "# Load the test batch\n",
        "test_batch = unpickle('test_batch')\n",
        "test_data = test_batch[b'data'].reshape(-1, 3, 32, 32).transpose(0, 2, 3, 1)\n",
        "test_labels = np.array(test_batch[b'labels'])"
      ],
      "metadata": {
        "id": "p8kLoBZDAZYm"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Training data shape: {train_data.shape}\")\n",
        "print(f\"Training labels shape: {train_labels.shape}\")\n",
        "print(f\"Test data shape: {test_data.shape}\")\n",
        "print(f\"Test labels shape: {test_labels.shape}\")"
      ],
      "metadata": {
        "id": "oqfPdJdzFW5W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c445c136-ee3a-418b-ee6b-a7c46b3ee872"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training data shape: (50000, 32, 32, 3)\n",
            "Training labels shape: (50000,)\n",
            "Test data shape: (10000, 32, 32, 3)\n",
            "Test labels shape: (10000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DataLoader Task 1"
      ],
      "metadata": {
        "id": "-caUY0lnGTAh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data_tensor = torch.tensor(train_data.transpose((0, 3, 1, 2)), dtype=torch.float) / 255.0\n",
        "train_labels_tensor = torch.tensor(train_labels, dtype=torch.long)\n",
        "test_data_tensor = torch.tensor(test_data.transpose((0, 3, 1, 2)), dtype=torch.float) / 255.0\n",
        "test_labels_tensor = torch.tensor(test_labels, dtype=torch.long)\n",
        "\n",
        "train_data_tensor = (train_data_tensor - 0.5) / 0.5\n",
        "test_data_tensor = (test_data_tensor - 0.5) / 0.5\n",
        "\n",
        "train_dataset = TensorDataset(train_data_tensor, train_labels_tensor)\n",
        "test_dataset = TensorDataset(test_data_tensor, test_labels_tensor)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
      ],
      "metadata": {
        "id": "ouvd-m33GdOx"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fetch the first batch from the train_loader\n",
        "first_batch_inputs, first_batch_labels = next(iter(train_loader))\n",
        "print(f\"Shape of the inputs in the first batch: {first_batch_inputs.shape}\")\n",
        "# Expected output: Shape of the inputs in the first batch: torch.Size([64, 3, 32, 32])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p4d98aHIpk5x",
        "outputId": "51123c3b-2908-4333-e222-4149aa1d79ef"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of the inputs in the first batch: torch.Size([64, 3, 32, 32])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "images, labels = next(iter(train_loader))\n",
        "\n",
        "print(f\"Batch images shape: {images.shape}\")\n",
        "print(f\"Batch labels shape: {labels.shape}\")\n",
        "\n",
        "# Visualize the first image in the batch\n",
        "plt.figure(figsize=(2, 2))\n",
        "plt.imshow(images[0].permute(1, 2, 0) * 0.5 + 0.5)  # Unnormalize\n",
        "plt.title(f\"Label: {labels[0]}\")\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xGiUdmEnJJBk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 249
        },
        "outputId": "298bd4c0-26a0-40a5-97a5-108d9cc51e54"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch images shape: torch.Size([64, 3, 32, 32])\n",
            "Batch labels shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 200x200 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# class IntermediateBlock(nn.Module):\n",
        "#     def __init__(self, in_channels, out_channels, num_convs=3):\n",
        "#         super(IntermediateBlock, self).__init__()\n",
        "#         # Define L independent convolutional layers.\n",
        "#         self.convs = nn.ModuleList([nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1) for _ in range(num_convs)])\n",
        "#         # Fully connected layer to compute weights from the input's channel means.\n",
        "#         self.fc = nn.Linear(in_channels, num_convs)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         # Compute the mean of each input channel.\n",
        "#         channel_means = x.mean([-2, -1])\n",
        "#         # Obtain weights for combining convolutional outputs.\n",
        "#         weights = F.softmax(self.fc(channel_means), dim=1)\n",
        "#         # Combine the convolutional layers' outputs according to computed weights.\n",
        "#         conv_outputs = torch.stack([conv(x) for conv in self.convs], dim=1)\n",
        "#         output = torch.sum(weights.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1) * conv_outputs, dim=1)\n",
        "#         return output\n",
        "\n",
        "# class OutputBlock(nn.Module):\n",
        "#     def __init__(self, in_channels, num_classes):\n",
        "#         super(OutputBlock, self).__init__()\n",
        "#         # Global average pooling to reduce each channel to a single value.\n",
        "#         self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "#         # Fully connected layer(s) to produce the logits from averaged channel values.\n",
        "#         self.fc = nn.Linear(in_channels, num_classes)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         x = self.avgpool(x)\n",
        "#         x = torch.flatten(x, 1)\n",
        "#         x = self.fc(x)\n",
        "#         return x\n",
        "\n",
        "# class CIFAR10Classifier(nn.Module):\n",
        "#     def __init__(self, num_blocks=2, in_channels=3, num_classes=10):\n",
        "#         super(CIFAR10Classifier, self).__init__()\n",
        "#         # Sequentially add intermediate blocks with increasing channels.\n",
        "#         self.blocks = nn.Sequential()\n",
        "#         out_channels = 64  # Adjust based on performance and complexity needs.\n",
        "#         for i in range(num_blocks):\n",
        "#             self.blocks.add_module(f\"block_{i+1}\", IntermediateBlock(in_channels if i == 0 else out_channels, out_channels))\n",
        "#         # Output block to produce final classification logits.\n",
        "#         self.output_block = OutputBlock(out_channels, num_classes)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         # Process input through intermediate blocks and output block.\n",
        "#         x = self.blocks(x)\n",
        "#         logits = self.output_block(x)\n",
        "#         return logits\n",
        "\n",
        "# # Initialize the model and print its architecture.\n",
        "# model = CIFAR10Classifier()\n",
        "# print(model)"
      ],
      "metadata": {
        "id": "ctlL6N6zqd-z"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model = model.to('cuda')"
      ],
      "metadata": {
        "id": "pV9oVesNu3_M"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class ImprovedIntermediateBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, num_convs=3):\n",
        "        super(ImprovedIntermediateBlock, self).__init__()\n",
        "        self.convs = nn.ModuleList([\n",
        "            nn.Sequential(\n",
        "                nn.Conv2d(in_channels if i == 0 else out_channels, out_channels, kernel_size=3, padding=1),\n",
        "                nn.BatchNorm2d(out_channels),\n",
        "                nn.ReLU(),\n",
        "                nn.Dropout2d(0.2)\n",
        "            ) for i in range(num_convs)\n",
        "        ])\n",
        "        self.fc = nn.Linear(in_channels, num_convs)\n",
        "\n",
        "    def forward(self, x):\n",
        "        channel_means = x.mean([-2, -1])\n",
        "        weights = F.softmax(self.fc(channel_means), dim=1)\n",
        "        conv_outputs = torch.stack([conv(x) for conv in self.convs], dim=1)\n",
        "        output = torch.sum(weights.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1) * conv_outputs, dim=1)\n",
        "        return output\n",
        "\n",
        "class EnhancedOutputBlock(nn.Module):\n",
        "    def __init__(self, in_channels, num_classes):\n",
        "        super(EnhancedOutputBlock, self).__init__()\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc1 = nn.Linear(in_channels, in_channels // 2)\n",
        "        self.fc2 = nn.Linear(in_channels // 2, num_classes)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "class CIFAR10Classifier(nn.Module):\n",
        "    def __init__(self, num_blocks=2, num_classes=10):\n",
        "        super(CIFAR10Classifier, self).__init__()\n",
        "        self.blocks = nn.Sequential()\n",
        "        in_channels = 3\n",
        "        out_channels = 64\n",
        "        for i in range(num_blocks):\n",
        "            self.blocks.add_module(f\"block_{i+1}\", ImprovedIntermediateBlock(in_channels, out_channels))\n",
        "            in_channels = out_channels\n",
        "\n",
        "        self.output_block = EnhancedOutputBlock(out_channels, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.blocks(x)\n",
        "        logits = self.output_block(x)\n",
        "        return logits\n",
        "\n",
        "model2 = CIFAR10Classifier()\n",
        "\n",
        "print(model2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sJDh6d0lBznq",
        "outputId": "92644547-c4db-4e39-b9ee-8e3c62885832"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CIFAR10Classifier(\n",
            "  (blocks): Sequential(\n",
            "    (block_1): ImprovedIntermediateBlock(\n",
            "      (convs): ModuleList(\n",
            "        (0): Sequential(\n",
            "          (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (2): ReLU()\n",
            "          (3): Dropout2d(p=0.2, inplace=False)\n",
            "        )\n",
            "        (1-2): 2 x Sequential(\n",
            "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (2): ReLU()\n",
            "          (3): Dropout2d(p=0.2, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (fc): Linear(in_features=3, out_features=3, bias=True)\n",
            "    )\n",
            "    (block_2): ImprovedIntermediateBlock(\n",
            "      (convs): ModuleList(\n",
            "        (0-2): 3 x Sequential(\n",
            "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (2): ReLU()\n",
            "          (3): Dropout2d(p=0.2, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (fc): Linear(in_features=64, out_features=3, bias=True)\n",
            "    )\n",
            "  )\n",
            "  (output_block): EnhancedOutputBlock(\n",
            "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "    (fc1): Linear(in_features=64, out_features=32, bias=True)\n",
            "    (fc2): Linear(in_features=32, out_features=10, bias=True)\n",
            "    (dropout): Dropout(p=0.5, inplace=False)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming `model2` is your CIFAR10Classifier instance and is already defined\n",
        "model2.to(device)\n",
        "optimizer = optim.Adam(model2.parameters(), lr=0.001)  # Adjust learning rate as needed\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Placeholder for dynamic plotting (simplified version)\n",
        "losses = []\n",
        "def update_plot(epoch, loss):\n",
        "    losses.append(loss)\n",
        "    plt.plot(losses, '-x')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Training Loss')\n",
        "    plt.pause(0.001)\n",
        "\n",
        "num_epochs = 10  # Adjust number of epochs as needed\n",
        "\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "    start_time = time.time()\n",
        "    running_loss = 0.0\n",
        "    total_batches = 0\n",
        "\n",
        "    for i, (inputs, labels) in enumerate(train_loader, 0):\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model2(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        total_batches += 1\n",
        "\n",
        "    avg_loss = running_loss / total_batches\n",
        "    update_plot(epoch, avg_loss)  # Update plot with average loss for this epoch\n",
        "    elapsed_time = time.time() - start_time\n",
        "\n",
        "    if epoch % 10 == 0 or epoch == 1:\n",
        "        print(f'Epoch {epoch}/{num_epochs} - Loss: {avg_loss:.4f} - Time: {elapsed_time:.2f}s')\n",
        "\n",
        "plt.show()  # Make sure to display the plot at the end\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        },
        "id": "OElZQTovZhAV",
        "outputId": "6aa42138-a224-4028-963e-bf8db48fdd98"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Given groups=1, weight of size [64, 64, 3, 3], expected input[64, 3, 32, 32] to have 64 channels, but got 3 channels instead",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-745e925ff951>\u001b[0m in \u001b[0;36m<cell line: 22>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-aa73a2b2ebf0>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-aa73a2b2ebf0>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mchannel_means\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchannel_means\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mconv_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mconv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mconv_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-aa73a2b2ebf0>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mchannel_means\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchannel_means\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mconv_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mconv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mconv_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 460\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    454\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m--> 456\u001b[0;31m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[1;32m    457\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size [64, 64, 3, 3], expected input[64, 3, 32, 32] to have 64 channels, but got 3 channels instead"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import torch.optim as optim\n",
        "\n",
        "# criterion = nn.CrossEntropyLoss()  # Cross-entropy loss\n",
        "# optimizer = optim.Adam(model.parameters(), lr=0.001)  # Adam optimizer with a learning rate of 0.001"
      ],
      "metadata": {
        "id": "kI4Eg70avALW"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# num_epochs = 20  # Number of epochs to train for\n",
        "\n",
        "# for epoch in range(num_epochs):\n",
        "#     running_loss = 0.0\n",
        "#     for i, data in enumerate(train_loader, 0):\n",
        "#         inputs, labels = data[0].to('cuda'), data[1].to('cuda')\n",
        "\n",
        "#         optimizer.zero_grad()  # Zero the parameter gradients\n",
        "\n",
        "#         outputs = model(inputs)  # Forward pass\n",
        "#         loss = criterion(outputs, labels)  # Compute loss\n",
        "#         loss.backward()  # Backward pass\n",
        "#         optimizer.step()  # Optimize\n",
        "\n",
        "#         running_loss += loss.item()\n",
        "#         if i % 2000 == 1999:  # Print average loss every 2000 mini-batches\n",
        "#             print('[%d, %5d] loss: %.3f' %\n",
        "#                   (epoch + 1, i + 1, running_loss / 2000))\n",
        "#             running_loss = 0.0\n",
        "\n",
        "# print('Finished Training')"
      ],
      "metadata": {
        "id": "j2eR3GkovpvX"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# correct = 0\n",
        "# total = 0\n",
        "# with torch.no_grad():  # No need to compute gradient during evaluation\n",
        "#     for data in test_loader:\n",
        "#         images, labels = data[0].to('cuda'), data[1].to('cuda')\n",
        "#         outputs = model(images)\n",
        "#         _, predicted = torch.max(outputs.data, 1)\n",
        "#         total += labels.size(0)\n",
        "#         correct += (predicted == labels).sum().item()\n",
        "\n",
        "# print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
        "#     100 * correct / total))\n"
      ],
      "metadata": {
        "id": "pXSCK2hNwHpg"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# model2 = CIFAR10Classifier().to(device)\n",
        "\n",
        "# learning_rate = 0.001\n",
        "# num_epochs = 30\n",
        "# criterion = nn.CrossEntropyLoss().to(device)\n",
        "# optimizer = optim.Adam(model2.parameters(), lr=learning_rate)\n",
        "\n",
        "# # Training loop\n",
        "# for epoch in range(num_epochs):\n",
        "#     model2.train()\n",
        "#     running_loss = 0.0\n",
        "#     correct, total = 0, 0\n",
        "#     for images, labels in train_loader:\n",
        "#         images, labels = images.to(device), labels.to(device)\n",
        "#         optimizer.zero_grad()\n",
        "#         outputs = model2(images)\n",
        "#         loss = criterion(outputs, labels)\n",
        "#         loss.backward()\n",
        "#         optimizer.step()\n",
        "\n",
        "#         running_loss += loss.item()\n",
        "#         _, predicted = torch.max(outputs.data, 1)\n",
        "#         total += labels.size(0)\n",
        "#         correct += (predicted == labels).sum().item()\n",
        "\n",
        "#     print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}, '\n",
        "#           f'Accuracy: {100 * correct / total:.2f}%')\n",
        "\n",
        "# # Evaluation\n",
        "# model2.eval()\n",
        "# correct, total = 0, 0\n",
        "# with torch.no_grad():\n",
        "#     for images, labels in test_loader:\n",
        "#         images, labels = images.to(device), labels.to(device)\n",
        "#         outputs = model2(images)\n",
        "#         _, predicted = torch.max(outputs.data, 1)\n",
        "#         total += labels.size(0)\n",
        "#         correct += (predicted == labels).sum().item()\n",
        "\n",
        "# print(f'Test Accuracy: {100 * correct / total:.2f}%')"
      ],
      "metadata": {
        "id": "zfrnKgL-Xnho"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Assuming the necessary imports are already in place\n",
        "\n",
        "# # Hyperparameters\n",
        "# learning_rate = 0.001\n",
        "# batch_size = 64  # Ensure this matches DataLoader configuration\n",
        "# num_epochs = 30\n",
        "\n",
        "# # Model, Loss, and Optimizer\n",
        "# model2 = CIFAR10Classifier().to(device)\n",
        "# criterion = nn.CrossEntropyLoss()\n",
        "# optimizer = optim.Adam(model2.parameters(), lr=learning_rate)\n",
        "\n",
        "# # Training Loop\n",
        "# for epoch in range(num_epochs):\n",
        "#     model2.train()\n",
        "#     total_loss = 0\n",
        "#     correct = 0\n",
        "#     total = 0\n",
        "\n",
        "#     for images, labels in train_loader:\n",
        "#         images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "#         optimizer.zero_grad()\n",
        "#         outputs = model2(images)\n",
        "#         loss = criterion(outputs, labels)\n",
        "#         loss.backward()\n",
        "#         optimizer.step()\n",
        "\n",
        "#         total_loss += loss.item()\n",
        "#         _, predicted = torch.max(outputs, 1)\n",
        "#         total += labels.size(0)\n",
        "#         correct += (predicted == labels).sum().item()\n",
        "\n",
        "#     print(f'Epoch {epoch+1}/{num_epochs}, Loss: {total_loss/len(train_loader):.4f}, '\n",
        "#           f'Accuracy: {correct/total:.4f}')\n",
        "\n",
        "# # Evaluation\n",
        "# model2.eval()\n",
        "# correct = 0\n",
        "# total = 0\n",
        "# with torch.no_grad():\n",
        "#     for images, labels in test_loader:\n",
        "#         images, labels = images.to(device), labels.to(device)\n",
        "#         outputs = model2(images)\n",
        "#         _, predicted = torch.max(outputs, 1)\n",
        "#         total += labels.size(0)\n",
        "#         correct += (predicted == labels).sum().item()\n",
        "\n",
        "# print(f'Test Accuracy: {correct/total:.4f}')\n",
        "\n"
      ],
      "metadata": {
        "id": "ojMz48HjE5ZT"
      },
      "execution_count": 17,
      "outputs": []
    }
  ]
}
# -*- coding: utf-8 -*-
"""Untitled1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1A5CWQcz6oDH6kA6uKSSk5ESsM3Ezx0t_
"""

from google.colab import drive

import numpy as np
import os
import pickle
import numpy as np
import matplotlib.pyplot as plt

import torch
from torch import nn
from torch import optim

import torch.nn.functional as F

import torchvision
import torchvision.transforms as transforms
from torchvision.transforms import ToTensor, Normalize, Compose

from torch.utils.data import TensorDataset, DataLoader

drive.mount('/content/drive')

dataset_path = '/content/drive/My Drive/100/'
os.chdir(dataset_path)

!tar -xvzf cifar-10-python.tar.gz

os.listdir('.')

def unpickle(file):
    with open(f'cifar-10-batches-py/{file}', 'rb') as fo:
        dict = pickle.load(fo, encoding='bytes')
    return dict

train_data = []
train_labels = []

# Load and append all training batches
for i in range(1, 6):
    batch = unpickle(f'data_batch_{i}')
    train_data.append(batch[b'data'])
    train_labels += batch[b'labels']

# Convert lists to numpy arrays for convenience
train_data = np.vstack(train_data).reshape(-1, 3, 32, 32).transpose(0, 2, 3, 1)
train_labels = np.array(train_labels)

# Load the test batch
test_batch = unpickle('test_batch')
test_data = test_batch[b'data'].reshape(-1, 3, 32, 32).transpose(0, 2, 3, 1)
test_labels = np.array(test_batch[b'labels'])

print(f"Training data shape: {train_data.shape}")
print(f"Training labels shape: {train_labels.shape}")
print(f"Test data shape: {test_data.shape}")
print(f"Test labels shape: {test_labels.shape}")

"""## DataLoader Task 1"""

train_data_tensor = torch.tensor(train_data.transpose((0, 3, 1, 2)), dtype=torch.float) / 255.0
train_labels_tensor = torch.tensor(train_labels, dtype=torch.long)
test_data_tensor = torch.tensor(test_data.transpose((0, 3, 1, 2)), dtype=torch.float) / 255.0
test_labels_tensor = torch.tensor(test_labels, dtype=torch.long)

train_data_tensor = (train_data_tensor - 0.5) / 0.5
test_data_tensor = (test_data_tensor - 0.5) / 0.5

train_dataset = TensorDataset(train_data_tensor, train_labels_tensor)
test_dataset = TensorDataset(test_data_tensor, test_labels_tensor)

train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)

images, labels = next(iter(train_loader))

print(f"Batch images shape: {images.shape}")
print(f"Batch labels shape: {labels.shape}")

# Visualize the first image in the batch
plt.figure(figsize=(2, 2))
plt.imshow(images[0].permute(1, 2, 0) * 0.5 + 0.5)  # Unnormalize
plt.title(f"Label: {labels[0]}")
plt.axis('off')
plt.show()

class IntermediateBlock(nn.Module):
    def __init__(self, in_channels, out_channels, num_convs=3):
        super(IntermediateBlock, self).__init__()
        # Define L independent convolutional layers.
        self.convs = nn.ModuleList([nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1) for _ in range(num_convs)])
        # Fully connected layer to compute weights from the input's channel means.
        self.fc = nn.Linear(in_channels, num_convs)

    def forward(self, x):
        # Compute the mean of each input channel.
        channel_means = x.mean([-2, -1])
        # Obtain weights for combining convolutional outputs.
        weights = F.softmax(self.fc(channel_means), dim=1)
        # Combine the convolutional layers' outputs according to computed weights.
        conv_outputs = torch.stack([conv(x) for conv in self.convs], dim=1)
        output = torch.sum(weights.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1) * conv_outputs, dim=1)
        return output

class OutputBlock(nn.Module):
    def __init__(self, in_channels, num_classes):
        super(OutputBlock, self).__init__()
        # Global average pooling to reduce each channel to a single value.
        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))
        # Fully connected layer(s) to produce the logits from averaged channel values.
        self.fc = nn.Linear(in_channels, num_classes)

    def forward(self, x):
        x = self.avgpool(x)
        x = torch.flatten(x, 1)
        x = self.fc(x)
        return x

class CIFAR10Classifier(nn.Module):
    def __init__(self, num_blocks=2, in_channels=3, num_classes=10):
        super(CIFAR10Classifier, self).__init__()
        # Sequentially add intermediate blocks with increasing channels.
        self.blocks = nn.Sequential()
        out_channels = 64  # Adjust based on performance and complexity needs.
        for i in range(num_blocks):
            self.blocks.add_module(f"block_{i+1}", IntermediateBlock(in_channels if i == 0 else out_channels, out_channels))
        # Output block to produce final classification logits.
        self.output_block = OutputBlock(out_channels, num_classes)

    def forward(self, x):
        # Process input through intermediate blocks and output block.
        x = self.blocks(x)
        logits = self.output_block(x)
        return logits

# Initialize the model and print its architecture.
model = CIFAR10Classifier()
print(model)

model = model.to('cuda')

import torch.optim as optim

criterion = nn.CrossEntropyLoss()  # Cross-entropy loss
optimizer = optim.Adam(model.parameters(), lr=0.001)  # Adam optimizer with a learning rate of 0.001

num_epochs = 20  # Number of epochs to train for

for epoch in range(num_epochs):
    running_loss = 0.0
    for i, data in enumerate(train_loader, 0):
        inputs, labels = data[0].to('cuda'), data[1].to('cuda')

        optimizer.zero_grad()  # Zero the parameter gradients

        outputs = model(inputs)  # Forward pass
        loss = criterion(outputs, labels)  # Compute loss
        loss.backward()  # Backward pass
        optimizer.step()  # Optimize

        running_loss += loss.item()
        if i % 2000 == 1999:  # Print average loss every 2000 mini-batches
            print('[%d, %5d] loss: %.3f' %
                  (epoch + 1, i + 1, running_loss / 2000))
            running_loss = 0.0

print('Finished Training')

correct = 0
total = 0
with torch.no_grad():  # No need to compute gradient during evaluation
    for data in test_loader:
        images, labels = data[0].to('cuda'), data[1].to('cuda')
        outputs = model(images)
        _, predicted = torch.max(outputs.data, 1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()

print('Accuracy of the network on the 10000 test images: %d %%' % (
    100 * correct / total))